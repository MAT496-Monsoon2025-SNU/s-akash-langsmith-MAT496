{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracing for Different Types of Runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Types of runs:\n",
    "- LLM: Invokes an LLM\n",
    "- Retriever: Retrieves documents from databases or other sources\n",
    "- Tool: Executes actions with function calls\n",
    "- Chain: Default type; combines multiple Runs into a larger process\n",
    "- Prompt: Hydrates a prompt to be used with an LLM\n",
    "- Parser: Extracts structured data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'choices': [{'message': {'role': 'assistant',\n",
       "    'content': 'What author are you looking for? When was the book written?'}}]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langsmith import traceable\n",
    "\n",
    "inputs = [\n",
    "  {\"role\": \"system\", \"content\": \"You are a librarian who helps people locate their favourite books in the library based on the genre or theme or writer of said book.\"},\n",
    "  {\"role\": \"user\", \"content\": \"I'm looking for a book written in the dystopian genre.\"},\n",
    "]\n",
    "\n",
    "output = {\n",
    "  \"choices\": [\n",
    "      {\n",
    "          \"message\": {\n",
    "              \"role\": \"assistant\",\n",
    "              \"content\": \"What author are you looking for? When was the book written?\"\n",
    "          }\n",
    "      }\n",
    "  ]\n",
    "}\n",
    "\n",
    "@traceable(\n",
    "  run_type=\"llm\",\n",
    "  metadata={\n",
    "    \"ls_provider\": \"openai\",\n",
    "    \"ls_model_name\": \"gpt-4\"\n",
    "  }\n",
    ")\n",
    "def chat_model(messages: list):\n",
    "  return output\n",
    "\n",
    "chat_model(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Streaming LLM Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'choices': [{'message': {'content': 'Hello! Please explain this gap in your resume, job applicant',\n",
       "     'role': 'assistant'}}]}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _reduce_chunks(chunks: list):\n",
    "    all_text = \"\".join([chunk[\"choices\"][0][\"message\"][\"content\"] for chunk in chunks])\n",
    "    return {\"choices\": [{\"message\": {\"content\": all_text, \"role\": \"assistant\"}}]}\n",
    "\n",
    "@traceable(\n",
    "    run_type=\"llm\",\n",
    "    metadata={\"ls_provider\": \"my_provider\", \"ls_model_name\": \"my_model\"},\n",
    "    reduce_fn=_reduce_chunks,\n",
    ")\n",
    "def my_streaming_chat_model(messages: list):\n",
    "    for chunk in [\"Hello! Please explain this gap in your resume, \" + messages[1][\"content\"]]:\n",
    "        yield {\n",
    "            \"choices\": [\n",
    "                {\n",
    "                    \"message\": {\n",
    "                        \"content\": chunk,\n",
    "                        \"role\": \"assistant\",\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "\n",
    "list(\n",
    "    my_streaming_chat_model(\n",
    "        [\n",
    "            {\"role\": \"system\", \"content\": \"You are an annoying interviewer. Please question the user.\"},\n",
    "            {\"role\": \"user\", \"content\": \"job applicant\"},\n",
    "        ],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retriever Runs + Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_content': '1984 by George Orwell',\n",
       "  'type': 'Document',\n",
       "  'metadata': {'library_section': 'Dystopian Novels'}},\n",
       " {'page_content': 'Fahrenheit 451 by Ray Bradbury',\n",
       "  'type': 'Document',\n",
       "  'metadata': {'library_section': 'Dystopian Novels'}},\n",
       " {'page_content': 'Brave New World by Aldous Huxley',\n",
       "  'type': 'Document',\n",
       "  'metadata': {'library_section': 'Dystopian Novels'}}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langsmith import traceable\n",
    "\n",
    "def _convert_docs(results):\n",
    "  return [\n",
    "      {\n",
    "          \"page_content\": r,\n",
    "          \"type\": \"Document\",\n",
    "          \"metadata\": {\"library_section\": \"Dystopian Novels\"}\n",
    "      }\n",
    "      for r in results\n",
    "  ]\n",
    "\n",
    "@traceable(\n",
    "    run_type=\"retriever\"\n",
    ")\n",
    "def retrieve_docs(query):\n",
    "  contents = [\"1984 by George Orwell\", \"Fahrenheit 451 by Ray Bradbury\", \"Brave New World by Aldous Huxley\"]\n",
    "  return _convert_docs(contents)\n",
    "\n",
    "retrieve_docs(\"Searching for dystopian novels:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool Calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 74\u001b[39m\n\u001b[32m     50\u001b[39m tools = [\n\u001b[32m     51\u001b[39m     {\n\u001b[32m     52\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mfunction\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     66\u001b[39m     }\n\u001b[32m     67\u001b[39m ]\n\u001b[32m     69\u001b[39m inputs = [\n\u001b[32m     70\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mYou are a helpful assistant.\u001b[39m\u001b[33m\"\u001b[39m},\n\u001b[32m     71\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mI am in Japan\u001b[39m\u001b[33m\"\u001b[39m}\n\u001b[32m     72\u001b[39m ]\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m \u001b[43mask_about_driving\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\MAT496-LangSmith\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py:693\u001b[39m, in \u001b[36mtraceable.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(langsmith_extra, *args, **kwargs)\u001b[39m\n\u001b[32m    689\u001b[39m         function_result = run_container[\u001b[33m\"\u001b[39m\u001b[33mcontext\u001b[39m\u001b[33m\"\u001b[39m].run(\n\u001b[32m    690\u001b[39m             run_with_otel_context\n\u001b[32m    691\u001b[39m         )\n\u001b[32m    692\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m693\u001b[39m         function_result = \u001b[43mrun_container\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    694\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    695\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    696\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    697\u001b[39m     _cleanup_traceback(e)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36mask_about_driving\u001b[39m\u001b[34m(inputs, tools)\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;129m@traceable\u001b[39m(run_type=\u001b[33m\"\u001b[39m\u001b[33mchain\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mask_about_driving\u001b[39m(inputs, tools):\n\u001b[32m     27\u001b[39m     response = call_openai(inputs, tools)\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m     tool_call_args = json.loads(\u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchoices\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtool_calls\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m.function.arguments)\n\u001b[32m     29\u001b[39m     location = tool_call_args[\u001b[33m\"\u001b[39m\u001b[33mlocation\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     30\u001b[39m     driving_side = tool_call_args[\u001b[33m\"\u001b[39m\u001b[33mdriving_side\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[31mTypeError\u001b[39m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "from langsmith import traceable\n",
    "from openai import OpenAI\n",
    "from typing import List, Optional\n",
    "import json\n",
    "\n",
    "openai_client = OpenAI()\n",
    "\n",
    "@traceable(\n",
    "  run_type=\"tool\"\n",
    ")\n",
    "def get_current_temperature(location: str, unit: str):\n",
    "    return 65 if unit == \"Fahrenheit\" else 17\n",
    "\n",
    "@traceable(run_type=\"llm\")\n",
    "def call_openai(\n",
    "    messages: List[dict], tools: Optional[List[dict]]\n",
    ") -> str:\n",
    "  return openai_client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=messages,\n",
    "    temperature=0,\n",
    "    tools=tools\n",
    "  )\n",
    "\n",
    "@traceable(run_type=\"chain\")\n",
    "def ask_about_the_weather(inputs, tools):\n",
    "  response = call_openai(inputs, tools)\n",
    "  tool_call_args = json.loads(response.choices[0].message.tool_calls[0].function.arguments)\n",
    "  location = tool_call_args[\"location\"]\n",
    "  unit = tool_call_args[\"unit\"]\n",
    "  tool_response_message = {\n",
    "    \"role\": \"tool\",\n",
    "    \"content\": json.dumps({\n",
    "        \"location\": location,\n",
    "        \"unit\": unit,\n",
    "        \"temperature\": get_current_temperature(location, unit),\n",
    "    }),\n",
    "    \"tool_call_id\": response.choices[0].message.tool_calls[0].id\n",
    "  }\n",
    "  inputs.append(response.choices[0].message)\n",
    "  inputs.append(tool_response_message)\n",
    "  output = call_openai(inputs, None)\n",
    "  return output\n",
    "\n",
    "tools = [\n",
    "    {\n",
    "      \"type\": \"function\",\n",
    "      \"function\": {\n",
    "        \"name\": \"get_current_temperature\",\n",
    "        \"description\": \"Get the current temperature for a specific location\",\n",
    "        \"parameters\": {\n",
    "          \"type\": \"object\",\n",
    "          \"properties\": {\n",
    "            \"location\": {\n",
    "              \"type\": \"string\",\n",
    "              \"description\": \"The city and state, e.g., Chennai, TN\"\n",
    "            },\n",
    "            \"unit\": {\n",
    "              \"type\": \"string\",\n",
    "              \"enum\": [\"Celsius\", \"Fahrenheit\"],\n",
    "              \"description\": \"The temperature unit to use. Infer this from the user's location.\"\n",
    "            }\n",
    "          },\n",
    "          \"required\": [\"location\", \"unit\"]\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "]\n",
    "inputs = [\n",
    "  {\"role\": \"system\", \"content\": \"You are an assistant for question-answering tasks.\"},\n",
    "  {\"role\": \"user\", \"content\": \"What is the weather today in Chennai, India?\"},\n",
    "]\n",
    "\n",
    "ask_about_the_weather(inputs, tools)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
